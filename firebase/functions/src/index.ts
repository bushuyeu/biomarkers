
/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Firebase imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
import {                                   // Import from Firebase Functions v2
  onDocumentCreated,                       //  â†³ Firestore onCreate trigger
} from "firebase-functions/v2/firestore";
import * as logger from "firebase-functions/logger"; // Cloudâ€‘native structured logging

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Admin SDK setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
import { initializeApp } from "firebase-admin/app";   // Initialise Admin SDK
import { getStorage } from "firebase-admin/storage";  // Access Cloud Storage

initializeApp();                           // Ensure Admin SDK is initialised exactly once

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Shared heavyâ€‘lifting logic (OCR + LLM) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
import {
  processDocumentFromStorage,              //  â†³ Does OCR, LLM parsing, Firestore writes
} from "./pipelines/processDocumentFromStorage"; // Local pipeline helper

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Firestore trigger v2 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
export const processDocumentJob = onDocumentCreated( // Export Cloud Function
  "processingQueue/{jobId}",              // Path pattern (wildcard for jobId)
  async (event) => {                      // Async handler
    const jobData = event.data?.data();   // Extract raw job document data
    if (!jobData) {                       // Guard against missing data
      logger.error("Job document empty â€“ should never happen"); // Log error
      return;                             // Exit early (no throw to avoid retry loop)
    }

    // Destructure required fields from job document
    const { path, tenantId, userId, fileId } = jobData as {
      path: string;                       // Full GCS path of uploaded file
      tenantId: string;                   // Tenant namespace
      userId: string;                     // Uploader UID
      fileId: string;                     // Short file identifier
    };

    logger.info("ðŸ“¥ Job picked up", { tenantId, fileId }); // Log start

    // Doubleâ€‘check that the object actually exists in GCS before processing
    const bucket = getStorage().bucket(); // Reference default bucket
    const file = bucket.file(path);       // Reference target file
    const [exists] = await file.exists(); // Check existence
    if (!exists) {                        // If object not found (should be rare)
      const msg = `File not found: ${path}`; // Compose error message
      logger.error(msg);                  // Log error
      await event.data!.ref.update({      // Update job doc with ERROR status
        status: "ERROR",
        error: msg,
        finishedAt: new Date(),
      });
      return;                             // Exit early â€“ no further processing
    }

    /* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Run OCR + LLM parsing inside try/catch â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    try {
      // Call shared helper; returns { biomarkers, testDate }
      const result = await processDocumentFromStorage(
        path,          // GCS path
        tenantId,      // Tenant namespace
        fileId,        // File identifier
        userId,        // Requesting UID
      );

      // Persist success status & any lightweight summary (optional)
      await event.data!.ref.update({
        status: "DONE",                   // Mark as completed
        summary: {                        // Minimal summary for frontâ€‘end
          biomarkers: result.biomarkers.length,
          testDate: result.testDate,
        },
        finishedAt: new Date(),           // Timestamp end
      });

      logger.info("âœ… Document processed", { fileId }); // Log success
    } catch (err) {
      // Handle any runtime errors from OCR / LLM helper
      logger.error("ðŸš¨ Document processing failed", err as Error); // Log error

      // Update job doc so UI can surface the failure
      await event.data!.ref.update({
        status: "ERROR",
        error: (err as Error).message,
        finishedAt: new Date(),
      });

      throw err;                           // Reâ€‘throw so Cloud Functions marks failure
    }
  },
);
